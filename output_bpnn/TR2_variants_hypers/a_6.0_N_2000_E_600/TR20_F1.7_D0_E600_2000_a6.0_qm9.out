STARTING AT Sat Dec 24 16:22:34 CET 2022
CUDA is available:  False
Random seed: 1000
7 18
Reading dataset
Shuffling and extracting from dataset (length: 10000)
Shuffling and extraction done
Calculating composition features
Composition features done
Calculating composition features
Composition features done
Creating datasets and dataloaders
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
Epoch 00351: reducing learning rate of group 0 to 1.0000e-04.
1900
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
nfeat: 406
Epoch 00390: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00152: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00696: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 4: 6.007202187187501 [MAE: 4.6764706668426586]
Test error minimum for model run no. 4:  8.71323280685878 [MAE: 5.834088673383209]
Epoch 00053: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00053: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 1: 5.1301895954966 [MAE: 4.053288966540573]
Test error minimum for model run no. 1:  8.305990577280255 [MAE: 5.674361976821667]
Epoch 00376: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00429: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00523: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00059: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00064: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00440: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 2: 5.444175433961264 [MAE: 4.288164176544866]
Test error minimum for model run no. 2:  8.161337172891777 [MAE: 5.585373250195975]
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 5: 5.7146074471370545 [MAE: 4.479838778595311]
Test error minimum for model run no. 5:  8.503784213369904 [MAE: 5.698629580613474]
Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00107: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00054: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00061: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00413: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 3: 6.35569082773852 [MAE: 4.919622023324699]
Test error minimum for model run no. 3:  9.014862484144743 [MAE: 6.038019948844785]
Epoch 00205: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00074: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00351: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 6: 5.901799413638758 [MAE: 4.605043185453542]
Test error minimum for model run no. 6:  8.440986231165525 [MAE: 5.81800572866573]
Epoch 00262: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00084: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00177: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00489: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00081: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00071: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00067: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 4: 5.9633503173743945 [MAE: 4.655897698242113]
Test error minimum for model run no. 4:  8.449029408490382 [MAE: 5.811295417199934]
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 7: 5.653272437494975 [MAE: 4.418779384187008]
Test error minimum for model run no. 7:  8.454125364760111 [MAE: 5.83944876821706]
Epoch 00341: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00058: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00357: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 5: 6.40280558886583 [MAE: 4.953624874558598]
Test error minimum for model run no. 5:  8.696065734205577 [MAE: 5.965630205449189]
Epoch 00266: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00103: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00056: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00071: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00357: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 8: 5.947201323896777 [MAE: 4.627250503877473]
Test error minimum for model run no. 8:  8.311000755392298 [MAE: 5.710122942482338]
Epoch 00381: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00053: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00104: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00067: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00565: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00129: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 6: 5.964116415762807 [MAE: 4.6469977283406845]
Test error minimum for model run no. 6:  8.455729558050185 [MAE: 5.812126699078453]
Epoch 00072: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00100: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00061: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 9: 5.493620117253511 [MAE: 4.304257185772787]
Test error minimum for model run no. 9:  8.390049315842433 [MAE: 5.715346705538891]
Epoch 00626: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00054: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00053: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00451: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 7: 5.275664113738235 [MAE: 4.139374083182406]
Test error minimum for model run no. 7:  8.191624132676655 [MAE: 5.6207898029704895]
Epoch 00122: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00074: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00081: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 10: 5.72400095502347 [MAE: 4.4998404212813226]
Test error minimum for model run no. 10:  8.380028731469379 [MAE: 5.8284656011347815]
E_max_2 =  600
Cutoff Radius =  6.0
Selected Radial Transform =  20.0
factor =  1.7
displacement =  0.0
dataset =  datasets/random-ch4-10k.extxyz
n_train =  2000
n_test =  2000
Train error averaged over all models Train RMSE:  5.729893956771297 [Train MAE: 4.483319843649239]
Test error averaged over all models Test RMSE: 8.480153982021866 [Test MAE: 5.7674502105277226]
FINISHED at Sat Dec 24 23:09:30 CET 2022
Epoch 00576: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00056: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00067: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 8: 5.399553494308442 [MAE: 4.24672340107761]
Test error minimum for model run no. 8:  8.006839337001647 [MAE: 5.652993069117908]
Epoch 00536: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00132: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00053: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 9: 5.592516349464016 [MAE: 4.3827139082313]
Test error minimum for model run no. 9:  8.382601579519608 [MAE: 5.749569636163571]
Epoch 00479: reducing learning rate of group 0 to 1.0000e-04.
Epoch 00136: reducing learning rate of group 0 to 1.0000e-05.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.
Epoch 00158: reducing learning rate of group 0 to 1.0000e-07.
Epoch 00091: reducing learning rate of group 0 to 1.0000e-08.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-09.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-10.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-11.
Epoch 00052: reducing learning rate of group 0 to 1.0000e-12.
Very small learning rate reached: 1e-12
Train error minimum for model run no. 10: 5.793270795956854 [MAE: 4.520753653431915]
Test error minimum for model run no. 10:  8.263561133847086 [MAE: 5.67467380473525]
E_max_2 =  600
Cutoff Radius =  6.0
Selected Radial Transform =  20.0
factor =  1.7
displacement =  0.0
dataset =  datasets/random-ch4-10k.extxyz
n_train =  2000
n_test =  2000
Train error averaged over all models Train RMSE:  5.732133293266696 [Train MAE: 4.480716051347477]
Test error averaged over all models Test RMSE: 8.392764111810793 [Test MAE: 5.758483381057722]
FINISHED at Sun Dec 25 01:41:30 CET 2022
